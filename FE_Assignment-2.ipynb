{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12c2173-45ee-465c-9542-154f872be29a",
   "metadata": {},
   "source": [
    "# Question-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69d0af-7097-4e7c-b7f7-c62eb62df9fa",
   "metadata": {},
   "source": [
    "The filter method is one of the techniques used in feature selection, which is a process of selecting a subset of the most relevant features (variables or attributes) from a larger set of features. The primary goal of feature selection is to improve the performance of machine learning models by reducing the dimensionality of the data and eliminating irrelevant or redundant features.\n",
    "\n",
    "The filter method works by evaluating the individual features independently of the machine learning algorithm that will be used for modeling. It involves the following steps:\n",
    "\n",
    "Feature Evaluation: Each feature is evaluated individually based on some criteria, and a score is assigned to each feature. The choice of evaluation criteria depends on the problem and the nature of the data. Common evaluation criteria include:\n",
    "\n",
    "Correlation: Measures the relationship between a feature and the target variable (e.g., Pearson correlation for continuous target variables, point-biserial correlation for binary target variables).\n",
    "Mutual Information: Measures the mutual dependence between a feature and the target variable.\n",
    "Chi-squared test: Tests the independence of a categorical feature and a categorical target variable.\n",
    "ANOVA F-statistic: Tests the difference in means of a numerical feature across different classes of a categorical target variable.\n",
    "Ranking Features: After evaluating each feature, they are ranked based on their scores. Features with higher scores are considered more important or relevant to the target variable.\n",
    "\n",
    "Selection of Top Features: The top-ranked features are selected for use in the machine learning model, while the lower-ranked features are discarded.\n",
    "\n",
    "The key advantage of the filter method is that it is computationally efficient and can quickly identify and select a subset of features without involving the machine learning model itself. However, it has some limitations:\n",
    "\n",
    "It does not consider interactions between features. Some features might be individually weak but become significant when combined with others.\n",
    "It may select irrelevant features if they have a high individual score, leading to suboptimal model performance.\n",
    "It does not adapt to the specific learning algorithm being used, so it may not consider the best feature subset for the given model.\n",
    "To address these limitations, other feature selection methods like wrapper methods and embedded methods are often used. Wrapper methods involve using a specific machine learning model to evaluate feature subsets, while embedded methods incorporate feature selection into the training process of the model itself. The choice of feature selection method depends on the specific problem and the dataset characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e211c-f316-4437-9175-e606a7975575",
   "metadata": {},
   "source": [
    "# Question-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae29535-8c8e-44fe-be13-1d294ddc4f5c",
   "metadata": {},
   "source": [
    "The Wrapper method and the Filter method are two different approaches to feature selection in machine learning. They differ in their underlying techniques and how they assess the importance of features. Here's a comparison of the two methods:\n",
    "\n",
    "Filter Method:\n",
    "\n",
    "Independence of ML Model: Filter methods evaluate features independently of the machine learning model that will be used for prediction. They do not involve the actual machine learning algorithm during the feature selection process.\n",
    "\n",
    "Feature Evaluation: Features are evaluated based on some statistical measure or scoring criterion, such as correlation, mutual information, chi-squared, or ANOVA. Each feature is assessed individually with respect to the target variable.\n",
    "\n",
    "Fast and Computationally Efficient: Filter methods are computationally efficient and are typically much faster than wrapper methods because they don't require training and evaluating a machine learning model for each feature subset.\n",
    "\n",
    "No Consideration of Feature Interactions: Filter methods do not consider interactions between features. They evaluate each feature in isolation, which may not capture the joint influence of features on the target variable.\n",
    "\n",
    "Limited to Univariate Analysis: Filter methods are univariate in nature, meaning they assess the importance of features one at a time. They may select irrelevant features if they have high individual scores but don't consider the combined impact of features.\n",
    "\n",
    "Wrapper Method:\n",
    "\n",
    "Incorporates ML Model: Wrapper methods use a specific machine learning model as part of the feature selection process. They wrap the feature selection process around the training and evaluation of the machine learning model.\n",
    "\n",
    "Feature Selection as a Search Problem: Wrapper methods treat feature selection as a search problem. They explore different feature subsets and assess their performance by training and testing a machine learning model on each subset.\n",
    "\n",
    "Iterative and Computationally Intensive: Wrapper methods can be computationally intensive and time-consuming because they involve multiple iterations of model training and testing for different feature subsets. They require more computational resources compared to filter methods.\n",
    "\n",
    "Considers Feature Interactions: Wrapper methods can capture feature interactions, as they evaluate feature subsets as a whole. This makes them better suited for problems where feature interactions are crucial for predictive performance.\n",
    "\n",
    "Model-Specific: The choice of the machine learning model used in the wrapper method can significantly impact the feature selection process. Different models may yield different feature subsets.\n",
    "\n",
    "Common techniques in the wrapper method include Forward Selection, Backward Elimination, and Recursive Feature Elimination (RFE), where the feature selection process is guided by the performance of the machine learning model on a validation dataset.\n",
    "\n",
    "In summary, the key difference is that the Filter method evaluates features independently of the machine learning model, while the Wrapper method integrates the machine learning model into the feature selection process, considering feature interactions and assessing feature subsets' performance through a search procedure. The choice between the two methods depends on the specific problem, available computational resources, and the importance of feature interactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a1748-ba3f-4c90-8f07-832ac795263d",
   "metadata": {},
   "source": [
    "# Question-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed85555-c6dd-4a07-a180-a074272c6cbe",
   "metadata": {},
   "source": [
    "Embedded feature selection methods are a group of techniques that perform feature selection as an integral part of the model building process. These methods embed the feature selection process within the training of a machine learning model. Here are some common techniques used in embedded feature selection:\n",
    "\n",
    "Lasso (L1 Regularization): Lasso is a linear regression technique that adds a penalty term to the linear regression cost function based on the absolute values of the feature coefficients. This penalty encourages many feature coefficients to become exactly zero, effectively selecting a subset of the most important features. Lasso is commonly used for feature selection in linear models.\n",
    "\n",
    "Ridge (L2 Regularization): Ridge regression adds a penalty term based on the squared values of feature coefficients. While it doesn't force coefficients to be exactly zero, it can help shrink less important coefficients towards zero. Ridge regularization can indirectly perform feature selection by reducing the impact of irrelevant features.\n",
    "\n",
    "Elastic Net: Elastic Net combines L1 (Lasso) and L2 (Ridge) regularization to strike a balance between feature selection and coefficient shrinkage. It can be useful when you want to select features while avoiding multicollinearity.\n",
    "\n",
    "Decision Trees and Random Forests: Decision trees and ensemble methods like Random Forests can be used for feature selection. They naturally rank features based on their importance in making splits or decisions within the tree. Features with higher importance scores are considered more relevant.\n",
    "\n",
    "Gradient Boosting Machines: Gradient Boosting models, like XGBoost, LightGBM, and CatBoost, can be used for feature selection. These models often include feature importance scores that help identify the most important features in the data.\n",
    "\n",
    "Recursive Feature Elimination (RFE): RFE is a technique that works with various machine learning algorithms. It recursively fits a model on the dataset, eliminates the feature with the lowest importance, and repeats the process until the desired number of features is reached. It's a backward selection method.\n",
    "\n",
    "L1 Regularized SVM: Support Vector Machines with L1 regularization can perform feature selection by promoting sparsity in the feature space. This is useful when you have high-dimensional data and want to select a subset of relevant features.\n",
    "\n",
    "Regularized Neural Networks: In deep learning, you can use regularized neural network architectures that incorporate L1 or L2 regularization to encourage feature sparsity and select important features.\n",
    "\n",
    "Genetic Algorithms: Genetic algorithms can be used to evolve feature subsets by combining and mutating different subsets to optimize the performance of a model. This is a more advanced technique that explores various feature combinations.\n",
    "\n",
    "Boruta Algorithm: The Boruta algorithm is designed specifically for feature selection in Random Forests. It compares the feature importance scores of the original features to those of a shuffled set of \"shadow\" features to determine which features are statistically significant.\n",
    "\n",
    "Permutation Importance: Permutation importance is a technique that evaluates feature importance by randomly permuting the values of a single feature and measuring the impact on model performance. Features with the most significant performance drops are considered important.\n",
    "\n",
    "These embedded feature selection techniques are advantageous because they consider the feature selection process during model training. They can lead to models that are more robust and optimized for the specific dataset, and they can often handle feature interactions more effectively than filter methods. The choice of method depends on the specific problem, dataset, and machine learning algorithm being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35393db2-9c5d-4a9a-95f8-e9573f1246c2",
   "metadata": {},
   "source": [
    "# Question-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69414a9-e007-4406-b9d8-354077cab4bb",
   "metadata": {},
   "source": [
    "While the Filter method for feature selection has its advantages, it also has some drawbacks and limitations. Here are some of the common drawbacks associated with using the Filter method:\n",
    "\n",
    "Independence of Features: Filter methods evaluate features independently, which means they do not consider the relationships or interactions between features. As a result, they may select features that, when considered together, would provide more valuable information than when evaluated individually. This limitation can lead to suboptimal feature subsets.\n",
    "\n",
    "Limited to Univariate Analysis: Filter methods are univariate in nature, meaning they assess the importance of features one at a time. They do not account for the joint influence of multiple features, which can be critical in many real-world applications where feature interactions play a significant role.\n",
    "\n",
    "Risk of Selecting Irrelevant Features: Filter methods may select features that have a high individual score based on the chosen evaluation criterion but are not actually relevant to the target variable. This can result in noise in the feature subset, potentially degrading model performance.\n",
    "\n",
    "No Adaptation to Model Choice: Filter methods do not adapt to the specific machine learning algorithm being used for modeling. The features selected based on one evaluation criterion may not be the most suitable for a different algorithm. Different algorithms may have different requirements and may benefit from different feature subsets.\n",
    "\n",
    "Inadequate for Complex Datasets: In complex datasets with high dimensionality and intricate relationships between features, filter methods may not effectively capture the underlying patterns. They may lead to suboptimal feature subsets or overlook essential information.\n",
    "\n",
    "Scalability Issues: For very high-dimensional datasets, evaluating and ranking all features using filter methods can be computationally expensive. This can result in longer feature selection times and increased computational resource requirements.\n",
    "\n",
    "Limited Ability to Handle Noisy Data: Filter methods do not inherently handle noisy data well. Noisy features may still receive high scores if their noise is misinterpreted as signal, potentially leading to poor model performance.\n",
    "\n",
    "Choice of Evaluation Metric: The effectiveness of the Filter method heavily relies on the choice of the evaluation metric or scoring criterion. Selecting an inappropriate metric can lead to suboptimal feature subsets.\n",
    "\n",
    "Lack of Dynamic Adaptation: Filter methods provide a static feature selection process. Once the features are selected, they are not updated as new data becomes available, which can be a limitation in dynamic environments.\n",
    "\n",
    "To address these limitations, other feature selection methods such as the Wrapper method and Embedded method (e.g., L1 regularization, decision trees) may be more suitable. These methods consider feature interactions, adapt to specific machine learning models, and provide a more holistic approach to feature selection. The choice of feature selection method should depend on the specific characteristics of the data and the goals of the modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484674c7-bf23-4fc6-920c-a6a08b9c2b18",
   "metadata": {},
   "source": [
    "# Question-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a813d8-eb1a-4286-8b19-48773ffde675",
   "metadata": {},
   "source": [
    "The choice between using the Filter method and the Wrapper method for feature selection depends on the specific characteristics of your data, your goals, and the computational resources available. There are situations where the Filter method is a more suitable choice over the Wrapper method:\n",
    "\n",
    "High-Dimensional Data: When dealing with high-dimensional datasets with a large number of features, the Filter method is often preferred because it is computationally efficient. It can quickly rank and select features without the need to train and evaluate a machine learning model for each feature subset, which can be time-consuming and resource-intensive in high-dimensional settings.\n",
    "\n",
    "Exploratory Data Analysis: In the initial stages of data analysis or when you want to gain insights into the relationships between individual features and the target variable, the Filter method can provide a quick and straightforward way to identify potentially relevant features. It helps in the preliminary exploration of the data before more computationally intensive feature selection methods are applied.\n",
    "\n",
    "Preprocessing Steps: The Filter method is often used as a preprocessing step before applying more complex feature selection or dimensionality reduction techniques. It can serve as a quick filter to reduce the dimensionality of the data and improve the efficiency of subsequent modeling steps.\n",
    "\n",
    "Feature Ranking: If your primary goal is to rank features based on their individual importance, the Filter method is suitable. It provides a ranking of features that can be useful for identifying the most informative features, even if you plan to use the Wrapper method or another feature selection method later.\n",
    "\n",
    "Stability in Results: Filter methods tend to produce stable results because they are not sensitive to variations in the choice of the machine learning model. This stability can be advantageous when you want consistent and interpretable feature selection outcomes.\n",
    "\n",
    "Interpretability: Filter methods can be more interpretable because they rely on simple statistical or information-theoretic measures to assess feature importance. This interpretability is valuable when you need to explain the selection of specific features to stakeholders or domain experts.\n",
    "\n",
    "Quick Iteration: In scenarios where you need to iterate quickly through different feature subsets and experiment with various feature selection criteria, the Filter method's speed and simplicity can be advantageous.\n",
    "\n",
    "Baseline Feature Selection: The Filter method can serve as a baseline for feature selection. You can start with a quick feature selection using filter criteria, and then, if necessary, explore more complex and model-specific techniques like the Wrapper method for further refinement.\n",
    "\n",
    "It's important to note that the choice between the Filter method and the Wrapper method is not mutually exclusive. In many cases, a combination of both methods can be effective. You can begin with the Filter method for quick initial feature selection and then use the Wrapper method to further refine the feature subset based on the chosen machine learning model. The specific situation, goals, and the nature of the data will guide your choice of feature selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00e336-4a9f-4222-a658-ed6947e905fc",
   "metadata": {},
   "source": [
    "# Question-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1a892-58f8-4a4d-b399-b4a79f12dd18",
   "metadata": {},
   "source": [
    "To choose the most pertinent attributes for a predictive model of customer churn in a telecom company using the Filter Method, you can follow these steps:\n",
    "\n",
    "Data Exploration:\n",
    "\n",
    "Start by thoroughly understanding the dataset. Explore and familiarize yourself with the features, the target variable (churn status), and the data distribution. This exploratory phase will help you gain insights into the dataset and the potential relevance of various features.\n",
    "\n",
    "Define Evaluation Criteria:\n",
    "\n",
    "Decide on the evaluation criteria you want to use for feature selection. Common criteria for filter methods in the context of customer churn prediction might include:\n",
    "\n",
    "Correlation with Churn: For numerical features, calculate the correlation coefficient (e.g., Pearson correlation) with the churn status.\n",
    "Mutual Information: Use mutual information or other information-theoretic metrics to measure the dependency between categorical features and churn.\n",
    "Chi-squared test: If you have categorical features, you can use the chi-squared test to evaluate their association with churn.\n",
    "Feature Scoring:\n",
    "\n",
    "Apply the chosen evaluation criteria to each feature in the dataset. Calculate the scores or statistics that represent the relevance of each feature to the target variable (churn). For example, compute correlation coefficients, mutual information scores, or chi-squared statistics.\n",
    "\n",
    "Feature Ranking:\n",
    "\n",
    "Rank the features based on their scores. Features with higher scores are considered more pertinent or relevant to predicting customer churn.\n",
    "\n",
    "Select Top Features:\n",
    "\n",
    "Choose a specific number of top-ranked features to include in your predictive model. The number of features you select can be based on domain knowledge, available computational resources, or experimentation. You may start with a relatively small subset and later adjust it based on model performance.\n",
    "\n",
    "Validate the Feature Subset:\n",
    "\n",
    "After selecting the top features, it's crucial to validate the feature subset by assessing the model's performance. You can use techniques such as cross-validation or hold-out validation to evaluate the model's predictive accuracy, F1-score, or other relevant metrics.\n",
    "\n",
    "Iterate if Necessary:\n",
    "\n",
    "If the initial feature subset does not yield satisfactory results, consider iterating the process. You can experiment with different feature selection criteria, evaluate the impact of including additional features, or explore interactions between features. The goal is to improve model performance and interpretability.\n",
    "\n",
    "Documentation and Reporting:\n",
    "\n",
    "Document the selected feature subset and the rationale behind the selection. Communicate the results and insights to stakeholders, including the reasons for choosing specific features and the potential implications for decision-making.\n",
    "\n",
    "Model Building:\n",
    "\n",
    "Finally, build the predictive model for customer churn using the selected feature subset. You can use various machine learning algorithms, such as logistic regression, decision trees, random forests, or gradient boosting, to create the model.\n",
    "\n",
    "Keep in mind that the choice of evaluation criteria and the number of features to select will depend on the specific dataset, business objectives, and the nature of customer churn in the telecom industry. Continuous monitoring and fine-tuning of the model, as well as periodic updates to the feature selection process, can help maintain the model's effectiveness over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd431c51-8ac4-4de7-ae0e-a308d6293cba",
   "metadata": {},
   "source": [
    "# Question-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09745dfd-ddfc-4fa9-a555-e3b6d40ecb92",
   "metadata": {},
   "source": [
    "Using the Embedded method for feature selection in a soccer match outcome prediction project involves incorporating feature selection into the model training process. Here's a step-by-step explanation of how to use the Embedded method to select the most relevant features for your model:\n",
    "\n",
    "Data Preprocessing:\n",
    "Begin by cleaning and preprocessing your dataset. This may involve handling missing values, encoding categorical variables, and standardizing or scaling numerical features as needed.\n",
    "\n",
    "Feature Engineering:\n",
    "Create any additional features that may be relevant for predicting soccer match outcomes. This could include derived statistics, historical performance metrics, or other domain-specific features.\n",
    "\n",
    "Feature Selection Models:\n",
    "Choose a machine learning algorithm that inherently supports feature selection as part of its training process. Some popular algorithms that can perform embedded feature selection include:\n",
    "\n",
    "L1 Regularized Models: Models that use L1 regularization, such as logistic regression or linear support vector machines (SVM), encourage sparsity in feature selection by adding a penalty term to the optimization function. Features with low coefficients become effectively excluded from the model.\n",
    "\n",
    "Decision Trees and Random Forests: Decision trees and ensemble methods like Random Forests can be used for embedded feature selection. Features that are less important for decision-making are given lower splits in the tree structure.\n",
    "\n",
    "Gradient Boosting Machines: Algorithms like XGBoost, LightGBM, and CatBoost provide feature importance scores during training. These scores can be used to select relevant features.\n",
    "\n",
    "Neural Networks with Regularization: Deep learning models, such as neural networks, can include L1 or L2 regularization to encourage feature sparsity.\n",
    "\n",
    "Training the Model:\n",
    "Train your selected machine learning model on the dataset. The model will automatically perform feature selection as it learns to predict soccer match outcomes. During this process, it will assign different levels of importance to each feature.\n",
    "\n",
    "Evaluate Feature Importance:\n",
    "After training the model, examine the feature importance scores assigned by the algorithm. These scores reflect the relevance of each feature in making predictions. Features with higher importance scores are considered more relevant.\n",
    "\n",
    "Thresholding and Feature Selection:\n",
    "Determine a threshold for feature importance that meets your model's performance objectives. Features with importance scores above this threshold are considered relevant and are selected for the final feature subset. You can set the threshold based on experimentation or domain knowledge.\n",
    "\n",
    "Validation and Iteration:\n",
    "Validate the performance of your model using the selected feature subset. Employ cross-validation or a separate validation dataset to ensure the model's predictive accuracy and generalizability. If necessary, iterate by adjusting the feature subset and threshold based on performance results.\n",
    "\n",
    "Model Deployment:\n",
    "Once you have selected the most relevant features and fine-tuned your model, you can deploy it to make predictions on new soccer matches. Ensure that you maintain regular monitoring and updates to account for changes in the data and feature importance.\n",
    "\n",
    "The Embedded method, by integrating feature selection into the model training process, allows you to discover the most pertinent features based on the inherent information gain or importance they provide to the model. This approach often results in a more tailored and optimized feature subset, which is crucial for improving the model's predictive accuracy in soccer match outcome prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62beb9a6-c1ff-404b-a02e-1eaedecc1741",
   "metadata": {},
   "source": [
    "# Question-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60767fdb-40e3-4111-94a0-055247f38879",
   "metadata": {},
   "source": [
    "Using the Wrapper method for feature selection in a project to predict house prices involves a systematic and iterative approach to selecting the best set of features for your predictor. Here's how you can use the Wrapper method for this task:\n",
    "\n",
    "Data Preprocessing:\n",
    "Start by cleaning and preprocessing your dataset, which includes handling missing values, encoding categorical variables, and normalizing or scaling numerical features as necessary.\n",
    "\n",
    "Split the Dataset:\n",
    "Divide your dataset into two sets: a training set and a validation set (or test set). The training set will be used for feature selection and model training, while the validation set will be used for evaluating the model's performance.\n",
    "\n",
    "Choose a Subset of Features:\n",
    "Initially, choose a subset of your available features to form the starting set. You can start with a small set of features or use all available features if the dataset is relatively small.\n",
    "\n",
    "Model Selection:\n",
    "Select a machine learning model that can be used for feature selection. Common choices include linear regression, decision trees, random forests, or gradient boosting models. The choice of model can depend on the nature of the data and the type of problem you are solving.\n",
    "\n",
    "Feature Selection Loop:\n",
    "Perform the following iterative process to find the best set of features:\n",
    "\n",
    "a. Train Model: Train your chosen machine learning model on the training set using the current set of features.\n",
    "\n",
    "b. Evaluate Model: Use the validation set to assess the model's predictive performance. You can use metrics like mean squared error (MSE) for regression tasks, which measures the model's ability to predict house prices accurately.\n",
    "\n",
    "c. Record Performance: Keep track of the model's performance (e.g., MSE) for the current feature subset.\n",
    "\n",
    "d. Feature Selection: Implement a feature selection strategy to decide which feature(s) to add or remove in the next iteration. Common strategies include:\n",
    "\n",
    "Forward Selection: Start with an empty set of features and add one feature at a time, selecting the feature that improves the model's performance the most.\n",
    "Backward Elimination: Begin with all available features and iteratively remove the feature that has the least impact on the model's performance.\n",
    "Recursive Feature Elimination (RFE): Use a ranking of feature importance (e.g., feature coefficients or feature importances) and remove the least important feature in each iteration.\n",
    "e. Stop Criterion: Define a stopping criterion, such as a maximum number of features, a certain level of performance improvement, or a specific number of iterations.\n",
    "\n",
    "Select the Best Feature Subset:\n",
    "After completing the feature selection loop, choose the set of features that resulted in the best model performance on the validation set. This is your final set of selected features.\n",
    "\n",
    "Model Training and Testing:\n",
    "Train your chosen machine learning model using the selected feature subset on the entire dataset (both training and validation sets). After training, assess the model's performance on a separate, unseen test dataset to ensure its generalization capabilities.\n",
    "\n",
    "Regular Model Monitoring and Maintenance:\n",
    "Regularly monitor the model's performance and the relevance of the selected features. Update the feature subset as needed to account for changes in the data or domain-specific knowledge.\n",
    "\n",
    "The Wrapper method, while more computationally intensive compared to the Filter method, can help you identify the most relevant features for predicting house prices by actively assessing the impact of features on model performance. This iterative approach allows you to fine-tune your feature subset and improve the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce733af5-f8b2-4973-89d5-6c5fe19cf9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
